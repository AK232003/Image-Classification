{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag Of Visual Words Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pylab as pl\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_clusters = 512 # Number of Clusters for k-means clustering for BOVW\n",
    "train_number = 1 # Number Of Batches to be included for training\n",
    "Labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'] #Can use this list to infer the class of that label since the given label is number Label[label] will give us the class/type of object."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpacking and Reading the images and labels of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='latin1')\n",
    "        imgs = dict['data'].reshape(len(dict['data']), 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "        labels = dict['labels']\n",
    "    return (imgs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_batches(train_number):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for i in range(1, train_number+1):\n",
    "        imgs1, labels1 = unpickle('./cifar-10-batches-py/data_batch_' + str(i))\n",
    "        imgs.append(imgs1)\n",
    "        labels.append(labels1)\n",
    "    \n",
    "    return imgs, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the descriptors of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptors(imgs):\n",
    "    extractor = cv2.SIFT_create()\n",
    "    descriptors = np.asarray([])\n",
    "\n",
    "    for img in imgs:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        kps, descs = extractor.detectAndCompute(gray, None)\n",
    "        if type(descs) == np.ndarray :\n",
    "            if descriptors.shape[0] == 0:\n",
    "                descriptors = descs\n",
    "            else:\n",
    "                descriptors = np.concatenate((descriptors, descs), axis=0)\n",
    "\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptors_for_data(imgs):\n",
    "    descriptors = np.asarray([])\n",
    "    for imgs1 in imgs:\n",
    "        if(descriptors.shape[0] == 0):\n",
    "            descriptors = get_descriptors(imgs1)\n",
    "        else:\n",
    "            descriptors = np.concatenate((descriptors, get_descriptors(imgs1)), axis = 0)\n",
    "\n",
    "    return descriptors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(descriptors):\n",
    "    kmeans = KMeans(n_clusters = number_clusters)\n",
    "    kmeans.fit(descriptors)\n",
    "\n",
    "    return kmeans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Bag Of KeyPoints/Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_histograms(imgs, words, number_clusters):\n",
    "    histograms = []\n",
    "    extractor = cv2.SIFT_create()\n",
    "    for img in imgs:\n",
    "        histogram = [0]*number_clusters\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        kps, descs = extractor.detectAndCompute(gray, None)\n",
    "\n",
    "        if(type(descs) == np.ndarray):\n",
    "            prediction = words.predict(descs)\n",
    "            for i in prediction:\n",
    "                histogram[i] = histogram[i] + 1\n",
    "        \n",
    "        histograms.append(histogram)\n",
    "\n",
    "    return np.asarray(histograms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the Test and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_test_data(words, imgs, labels, number_clusters):\n",
    "    training_set = np.asarray([])\n",
    "    training_labels = []\n",
    "    test_set = np.asarray([])\n",
    "\n",
    "    for imgs1, labels1 in zip(imgs, labels):\n",
    "        if(training_set.shape[0] == 0):\n",
    "            training_set = build_histograms(imgs1, words, number_clusters)\n",
    "            training_labels = labels1\n",
    "        else:\n",
    "            training_set = np.concatenate((training_set, build_histograms(imgs1)), axis = 0)\n",
    "            training_labels.extend(labels1)\n",
    "    \n",
    "    test_imgs, test_labels = unpickle('./cifar-10-batches-py/test_batch')\n",
    "    test_set = build_histograms(test_imgs, words, number_clusters)\n",
    "\n",
    "    return training_set, np.asarray(training_labels), test_set, np.asarray(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(svm_c, svm_kernel, svm_gamma, training_set, training_labels):\n",
    "    model = svm.SVC(C = svm_c, kernel = svm_kernel, gamma = svm_gamma)\n",
    "    model.fit(training_set, training_labels)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = load_data_batches(train_number)\n",
    "descriptors = get_descriptors_for_data(imgs)\n",
    "words = get_words(descriptors)\n",
    "training_set, train_labels, test_set, test_labels = get_training_test_data(words, imgs, labels, number_clusters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "# 1. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(0.01, 'linear', 0.01, training_set, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2676"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = model.score(test_set, test_labels)\n",
    "accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train1(max_iter, training_set, training_labels):\n",
    "    model = LogisticRegression(max_iter = 1000)\n",
    "    model.fit(training_set, training_labels)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = train1(1000, training_set, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2505"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy1 = model1.score(test_set, test_labels)\n",
    "accuracy1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(n_neighbors, training_set, training_labels):\n",
    "    model = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "    model.fit(training_set, training_labels)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = train2(5, training_set, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1218"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy2 = model2.score(test_set, test_labels)\n",
    "accuracy2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
